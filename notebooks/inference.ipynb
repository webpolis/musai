{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "from miditok import REMI, MMM\n",
    "from torch.nn import functional as F\n",
    "from importlib import reload\n",
    "\n",
    "os.chdir('/home/nico/dev/projects/ai/musai')\n",
    "\n",
    "sys.path.append('./src/tools')\n",
    "sys.path.append('./src/model')\n",
    "\n",
    "import tokenizer\n",
    "\n",
    "reload(tokenizer)\n",
    "\n",
    "from tokenizer import get_tokenizer, parse_bpe_tokens, TOKEN_PARAMS_NAME\n",
    "\n",
    "PROJ_NAME = 'all'\n",
    "IS_BPE = False\n",
    "TOKENS_PATH = f\"/media/nico/nvme/data/tokens/tmp{'/bpe' if IS_BPE else ''}\"\n",
    "TOKENS_FILE_PATHS = list(Path(TOKENS_PATH).glob('*.json'))\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CTX_LEN = 1024\n",
    "PRECISION = 'bf16'\n",
    "\n",
    "os.environ['RWKV_T_MAX'] = str(CTX_LEN)\n",
    "os.environ['RWKV_FLOAT_MODE'] = PRECISION\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer(params=f'{TOKENS_PATH}/{TOKEN_PARAMS_NAME}')\n",
    "\n",
    "Path(f'./out/{PROJ_NAME}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ORIG_VOCAB_SIZE = len(TOKENIZER.vocab)\n",
    "BPE_VOCAB_SIZE = int(ORIG_VOCAB_SIZE * 1.25)\n",
    "\n",
    "(ORIG_VOCAB_SIZE, BPE_VOCAB_SIZE, len(TOKENIZER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EMBED = 768\n",
    "N_LAYER = 12\n",
    "\n",
    "params = {\n",
    "    'ctx_len': CTX_LEN,\n",
    "    'n_embd': N_EMBED,\n",
    "    'n_layer': N_LAYER,\n",
    "}\n",
    "\n",
    "params_obj = namedtuple('RWKVParams', params.keys())(*params.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAX_ITER = CTX_LEN\n",
    "\n",
    "# this is where we introduce some randomness\n",
    "NOISE_LEVEL = 0.55\n",
    "NOISE_FREQ = 10\n",
    "PHASE = 0\n",
    "\n",
    "\n",
    "def gen_sin_wave(total_iterations, min_value, max_value, noise_scale, noise_frequency, main_phase):\n",
    "    \"\"\"\n",
    "    Generate a sinusoidal wave with optional noise.\n",
    "\n",
    "    Args:\n",
    "        total_iterations (int): The total number of iterations.\n",
    "        min_value (float): The minimum value of the wave.\n",
    "        max_value (float): The maximum value of the wave.\n",
    "        noise_scale (float): The scale factor for the noise.\n",
    "        noise_frequency (float): The frequency of the noise wave.\n",
    "        main_phase (float): The phase of the main wave.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated wave values.\n",
    "\n",
    "    \"\"\"\n",
    "    progress = np.linspace(0, 1, total_iterations)\n",
    "    main_wave = np.sin(2 * np.pi * progress + main_phase)\n",
    "    noise_wave = np.sin(2 * np.pi * noise_frequency * progress - main_phase / 2)\n",
    "    noise = noise_scale * noise_wave\n",
    "    values = min_value + (max_value - min_value) * \\\n",
    "        (1 + main_wave) / 2 + noise\n",
    "\n",
    "    np.clip(values, min_value, max_value, out=values)\n",
    "\n",
    "    return values.tolist()\n",
    "\n",
    "\n",
    "temp_values = gen_sin_wave(MAX_ITER, 0.05, 0.25, NOISE_LEVEL, NOISE_FREQ, PHASE)\n",
    "top_p_values = gen_sin_wave(MAX_ITER, 0.65, 0.999, NOISE_LEVEL, NOISE_FREQ*2, PHASE+6)\n",
    "\n",
    "plt.plot(temp_values)\n",
    "plt.plot(top_p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runner\n",
    "\n",
    "reload(runner)\n",
    "\n",
    "from runner import RWKV\n",
    "import types\n",
    "\n",
    "SEED = random.randint(1000, 10000)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "args = types.SimpleNamespace()\n",
    "args.RUN_DEVICE = \"cuda\"\n",
    "args.FLOAT_MODE = PRECISION\n",
    "args.map_location = 'cpu'\n",
    "args.base_model = f'/home/nico/dev/projects/ai/musai/dist/main_1.pth'\n",
    "args.n_layer = params['n_layer']\n",
    "args.n_embd = params['n_embd']\n",
    "args.ctx_len = int(params['ctx_len'])\n",
    "args.vocab_size = len(TOKENIZER)\n",
    "args.head_size_a = 64\n",
    "\n",
    "model_rnn = RWKV(args)\n",
    "model_rnn.to(torch.bfloat16).cuda()\n",
    "\n",
    "model_rnn.load_state_dict(torch.load(args.base_model, map_location='cpu'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from miditoolkit import MidiFile\n",
    "\n",
    "init_state = None\n",
    "out_tokens = []\n",
    "tokens_file_paths = list(Path(TOKENS_PATH).glob('*.json'))\n",
    "\n",
    "random.shuffle(tokens_file_paths)\n",
    "\n",
    "# SAMPLE_TOKENS_FILE = f'/home/nico/data/ai/models/midi/{PROJ_NAME}/8f33606fa1a6040e5ba230ea7bff8546_mid.json'\n",
    "\n",
    "token_ids = json.load(open(tokens_file_paths[0]))['ids']\n",
    "# token_ids = json.load(open(SAMPLE_TOKENS_FILE))['ids']\n",
    "# sample = TOKENIZER.midi_to_tokens(MidiFile('examples/2023-06-11T12-59-05-739764.mid'))\n",
    "# token_ids = sample.ids\n",
    "max_seq = CTX_LEN if len(token_ids) >= CTX_LEN else len(token_ids)\n",
    "init_tokens = token_ids[:max_seq]\n",
    "print(init_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from runner import sample_logits, repetition_penalty  # Ensure this is properly defined in your runner module\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# Initialize the model in evaluation mode\n",
    "model_rnn.eval()\n",
    "\n",
    "# Initialize the buffer with the initial tokens, ensuring it does not exceed context_length\n",
    "context_length = 1024  # Define your context length\n",
    "buffer = deque(maxlen=context_length)\n",
    "for token in init_tokens[-context_length:]:\n",
    "    buffer.append(token)\n",
    "\n",
    "max_iterations = MAX_ITER  # Define your maximum number of iterations\n",
    "\n",
    "# Container for generated tokens\n",
    "generated_tokens = list(buffer)\n",
    "\n",
    "# Convert buffer to tensor\n",
    "input_ids = torch.tensor([list(buffer)], dtype=torch.long).cuda()\n",
    "print(f\"Input shape: {input_ids.shape}\")  # Should be [1, 1024]\n",
    "\n",
    "# Define pad token IDs to ignore (replace with actual pad token IDs)\n",
    "pad_token_ids = [0]  # Example: [0, 50256]\n",
    "out_tokens = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial forward pass\n",
    "    init_out = model_rnn.forward(input_ids)  # Shape: [1, 1024, vocab_size]\n",
    "    print(f\"Initial output shape: {init_out.shape}\")\n",
    "\n",
    "    # Ignore padding by setting their logits to -inf\n",
    "    init_out[0, :, pad_token_ids] = -float('inf')\n",
    "\n",
    "    # Sample the first token from the last position\n",
    "    logits = init_out[0, -1, :].cpu()  # Move to CPU for sampling if necessary\n",
    "    out_token = sample_logits(logits, temperature=0.25, top_k=50)  # Example values\n",
    "\n",
    "    # Append to generated tokens\n",
    "    generated_tokens.append(out_token)\n",
    "\n",
    "    # Add the first token to out_tokens for tracking\n",
    "    out_tokens.append(out_token)\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm(range(max_iterations), desc=\"Generating tokens\", unit=\"token\") as pbar:\n",
    "        for i in pbar:\n",
    "            # Update buffer with the new token\n",
    "            buffer.append(out_token)\n",
    "            generated_tokens.append(out_token)\n",
    "\n",
    "            # Convert buffer to tensor\n",
    "            input_ids = torch.tensor([list(buffer)], dtype=torch.long).cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model_rnn.forward(input_ids)  # Shape: [1, 1024, vocab_size]\n",
    "\n",
    "            # Ignore padding\n",
    "            output[0, :, pad_token_ids] = -float('inf')\n",
    "\n",
    "            # Apply repetition penalty if desired\n",
    "            # Uncomment and adjust the following lines if repetition_penalty is needed\n",
    "            # output = repetition_penalty(\n",
    "            #     output, generated_tokens, [4, 5, 6, 7], \n",
    "            #     repetition_penalty=1.25, \n",
    "            #     seq_len=256, \n",
    "            #     decay_factor=0.8\n",
    "            # )\n",
    "\n",
    "            # Get logits for the last token\n",
    "            logits = output[0, -1, :].cpu()\n",
    "\n",
    "            # Define temperature and top_k/top_p for the current step\n",
    "            current_temp = temp_values[i] if 'temp_values' in locals() else 1.0  # Replace with your logic\n",
    "            current_top_k = top_k_values[i] if 'top_k_values' in locals() else 50   # Replace with your logic\n",
    "            current_top_p = top_p_values[i] if 'top_p_values' in locals() else None  # Replace with your logic\n",
    "\n",
    "            # Sample the next token\n",
    "            out_token = sample_logits(\n",
    "                logits, \n",
    "                temperature=current_temp, \n",
    "                top_k=current_top_k, \n",
    "                top_p=current_top_p\n",
    "            )\n",
    "\n",
    "            # Append to generated tokens\n",
    "            generated_tokens.append(out_token)\n",
    "            out_tokens.append(out_token)\n",
    "\n",
    "            # Update the tqdm progress bar with the latest token\n",
    "            pbar.set_postfix(token=out_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'out/{PROJ_NAME}/{d}.mid'\n",
    "fname_orig = f'out/{PROJ_NAME}/{d}_orig.mid'\n",
    "\n",
    "TOKENIZER(out_tokens).dump_midi(fname)\n",
    "TOKENIZER(token_ids).dump_midi(fname_orig)\n",
    "\n",
    "[fname, fname_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "324c59c51086f4574d8cdca1e3c0b1230dd2abd272c806cb05bd1db673024182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
